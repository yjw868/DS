{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T09:09:49.320785Z",
     "start_time": "2018-02-21T09:09:49.309229Z"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston\n",
    "#from sklearn.model_selection import cross_val_predict, cross_validate, cross_val_score, train_test_split, KFold\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from itertools import combinations\n",
    "import recordlinkage\n",
    "from recordlinkage.datasets import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Project 1\n",
    "\n",
    "## Problem statement:\n",
    "Identify duplicated customers/accounts among multiple data sources. For example we have same customer but under different names in SalesForce, Finance system, CRM system.It is very difficult to get a meaningful analysis for our sales team, finance team, marketing team.\n",
    "    \n",
    "    \n",
    "## Data\n",
    "This is a common issues with medical care for the healthy department and government as well. I will use the data publised by Freely Extensible Biomedical Record Linkage (FEBRL) project.\n",
    "      \n",
    "http://recordlinkage.readthedocs.io/en/latest/ref-datasets.html\n",
    "\n",
    "http://users.cecs.anu.edu.au/~Peter.Christen/Febrl/febrl-0.3/febrldoc-0.3/manual.html  \n",
    "    \n",
    "    \n",
    "    \n",
    "## Hypothesis\n",
    "There are duplicated records in the data sources. The expected result would be any duplicated record by removed or linked.\n",
    "    \n",
    "## Potential API or Lib can help\n",
    "\n",
    "- sklearn\n",
    "- Record linkage toolkit (used this before)\n",
    "- Levenshtin distance (used fuzzyWuzzy before)\n",
    "- Builclassifier dedup lib\n",
    "- Nazca lib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T09:18:35.765167Z",
     "start_time": "2018-02-21T09:18:35.729326Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>street_number</th>\n",
       "      <th>address_1</th>\n",
       "      <th>address_2</th>\n",
       "      <th>suburb</th>\n",
       "      <th>postcode</th>\n",
       "      <th>state</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>soc_sec_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rec-1496-org</th>\n",
       "      <td>mitchell</td>\n",
       "      <td>green</td>\n",
       "      <td>7</td>\n",
       "      <td>wallaby place</td>\n",
       "      <td>delmar</td>\n",
       "      <td>cleveland</td>\n",
       "      <td>2119</td>\n",
       "      <td>sa</td>\n",
       "      <td>19560409</td>\n",
       "      <td>1804974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-552-dup-3</th>\n",
       "      <td>harley</td>\n",
       "      <td>mccarthy</td>\n",
       "      <td>177</td>\n",
       "      <td>pridhamstreet</td>\n",
       "      <td>milton</td>\n",
       "      <td>marsden</td>\n",
       "      <td>3165</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19080419</td>\n",
       "      <td>6089216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              given_name   surname street_number      address_1 address_2  \\\n",
       "rec_id                                                                      \n",
       "rec-1496-org    mitchell     green             7  wallaby place    delmar   \n",
       "rec-552-dup-3     harley  mccarthy           177  pridhamstreet    milton   \n",
       "\n",
       "                  suburb postcode state date_of_birth soc_sec_id  \n",
       "rec_id                                                            \n",
       "rec-1496-org   cleveland     2119    sa      19560409    1804974  \n",
       "rec-552-dup-3    marsden     3165   nsw      19080419    6089216  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA\n",
    "# http://recordlinkage.readthedocs.io/en/latest/ref-datasets.html\n",
    "\"\"\"This data set contains 5000 records (2000 originals and 3000 duplicates), with a maximum of 5 duplicates based on \n",
    "one original record (and a Zipf distribution of duplicate records). Distribution of duplicates: 168 originals records \n",
    "have 5 duplicate records 161 originals records have 4 duplicate records 212 originals records have 3 duplicate records \n",
    "256 originals records have 2 duplicate records 368 originals records have 1 duplicate record 1835 originals records have \n",
    "no duplicate record\"\"\"\n",
    "\n",
    "df= load_febrl3()\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T09:26:22.444083Z",
     "start_time": "2018-02-21T09:26:22.439080Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['given_name',\n",
       " 'surname',\n",
       " 'street_number',\n",
       " 'address_1',\n",
       " 'address_2',\n",
       " 'suburb',\n",
       " 'postcode',\n",
       " 'state',\n",
       " 'date_of_birth']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The features are:\")\n",
    "[x for x in df.columns[0:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T09:19:42.628997Z",
     "start_time": "2018-02-21T09:19:42.625334Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target is soc_sec_id\n"
     ]
    }
   ],
   "source": [
    "print(\"The target is {}\".format(df.columns[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T09:26:06.040925Z",
     "start_time": "2018-02-21T09:26:05.407157Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             given_name  surname  date_of_birth  suburb  \\\n",
      "rec_id         rec_id                                                     \n",
      "rec-1740-dup-2 rec-1740-org           1      1.0              1       1   \n",
      "rec-1596-dup-0 rec-1596-org           1      1.0              1       0   \n",
      "rec-1725-dup-4 rec-1725-org           1      1.0              1       0   \n",
      "\n",
      "                             state  address_1  \n",
      "rec_id         rec_id                          \n",
      "rec-1740-dup-2 rec-1740-org      1        1.0  \n",
      "rec-1596-dup-0 rec-1596-org      1        0.0  \n",
      "rec-1725-dup-4 rec-1725-org      1        0.0  \n",
      "3483\n"
     ]
    }
   ],
   "source": [
    "# Indexation step\n",
    "pcl = recordlinkage.BlockIndex(on='given_name')\n",
    "pairs = pcl.index(df)\n",
    "\n",
    "# Comparison step\n",
    "compare_cl = recordlinkage.Compare()\n",
    "\n",
    "compare_cl.exact('given_name', 'given_name', label='given_name')\n",
    "compare_cl.string('surname', 'surname', method='jarowinkler', threshold=0.85, label='surname')\n",
    "compare_cl.exact('date_of_birth', 'date_of_birth', label='date_of_birth')\n",
    "compare_cl.exact('suburb', 'suburb', label='suburb')\n",
    "compare_cl.exact('state', 'state', label='state')\n",
    "compare_cl.string('address_1', 'address_1', threshold=0.85, label='address_1')\n",
    "\n",
    "features = compare_cl.compute(pairs, df)\n",
    "\n",
    "# Classification step\n",
    "matches = features[features.sum(axis=1) > 3]\n",
    "\n",
    "print(matches.head(3))\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Project 2\n",
    "\n",
    "## Problem statement:\n",
    "Identify whether a potential customer will churn when their current contract is ended. It is more expensive to acquire a new customer than to keep existing one from leaving.I am hoping to use this model to build useful tool to predict customer life time value, help sales team to allocate resource to prevent customer leaving.\n",
    "    \n",
    "    \n",
    "    \n",
    "## Data\n",
    "Using the telecom churn data provided by kaggle\n",
    "    \n",
    "https://www.kaggle.com/becksddf/churn-in-telecoms-dataset/version/1.\n",
    "    \n",
    "https://bigml.com/user/francisco/gallery/dataset/5163ad540c0b5e5b22000383\n",
    "    \n",
    "    \n",
    "## Hypothesis\n",
    "Predict whether a given customer will churn or retain based on all the features.\n",
    "    \n",
    "## Potential API or Lib can help\n",
    "- sklearn\n",
    "- ...\n",
    "- Telecom Churn Notebook https://www.kaggle.com/arpitharavi/telecom-churn-notebook\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T09:09:49.358395Z",
     "start_time": "2018-02-21T09:09:49.265Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#EDA\n",
    "tele = pd.read_csv('./data/bigml.csv')\n",
    "tele.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T09:09:49.358896Z",
     "start_time": "2018-02-21T09:09:49.265Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The features are:\")\n",
    "[x for x in tele.columns[:-1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T09:09:49.359896Z",
     "start_time": "2018-02-21T09:09:49.266Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The target is {}\".format(tele.columns[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T09:09:49.360397Z",
     "start_time": "2018-02-21T09:09:49.267Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tele.describe()"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/012c72ddf96243926e376b568708f0b0"
  },
  "celltoolbar": "Slideshow",
  "gist": {
   "data": {
    "description": "Projects/Final Project PT1 - DUE 21-02/Yijie Final Project PT1.ipynb",
    "public": true
   },
   "id": "012c72ddf96243926e376b568708f0b0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
